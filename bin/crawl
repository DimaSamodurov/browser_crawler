#!/usr/bin/env ruby

require "bundler/setup"
require "crawler"

options = Crawler::Options.parse_args

if options[:url]
  engine = Crawler::Engine.new(
    max_pages: options[:max_pages],
    save_screenshots_to: options[:screenshots_path],
    username: options[:username],
    password: options[:password],
    window_width: options[:window_width],
    window_height: options[:window_height])

  engine.extract_links(url: options[:url]) if options[:url]
  File.write(options[:output], engine.report.to_yaml)
  puts "Report is saved to #{options[:output_links]}."

  puts "Total visited pages: #{engine.visited_pages.count}."
end

if options[:screenshots_path]
  template = File.read(options[:index_template]) if options[:index_template]
  indexer = Crawler::Followups::ScreenshotsIndexer.new(template: template)
  file = indexer.index_screenshots(options[:screenshots_path])
  puts "Screenshots index is saved to '#{file}'."
end
